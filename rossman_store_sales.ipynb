{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# /home/coenraadmiddel/miniconda3/envs/venvRossmann\n",
    "import os\n",
    "import wget\n",
    "from pathlib import Path\n",
    "\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Using {}\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/coenraadmiddel/Documents/RossmannStoreSales/Input/'\n",
    "trainset_name = 'rossman-store-sales'\n",
    "path_train = path+'train.csv'\n",
    "path_test = path+'test.csv'\n",
    "path_store = path+'store.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.read_csv(path_store)\n",
    "store.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path_train, parse_dates=[2])\n",
    "test = pd.read_csv(path_test, parse_dates=[2])\n",
    "# test.fillna(value = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data with store \n",
    "train = pd.merge(train, store, on='Store')\n",
    "test = pd.merge(test, store, on='Store')\n",
    "\n",
    "# split the last 6 weeks data as hold-out set (idea from Gert https://www.kaggle.com/c/rossmann-store-sales/discussion/18024)\n",
    "train = train.sort_values(['Date'],ascending = False)\n",
    "train_total = train.copy()\n",
    "\n",
    "split_index = 6*7*1115\n",
    "valid = train[:split_index] \n",
    "train = train[split_index:]\n",
    "\n",
    "# only use train of Sales>0 and Open is 1\n",
    "valid = valid[(valid.Open != 0)&(valid.Sales >0)]\n",
    "train = train[(train.Open != 0)&(train.Sales >0)]\n",
    "train_total = train_total[(train_total.Open != 0)&(train_total.Sales >0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log1p(train['Sales']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "# print(valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check distribution of sales in train set\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "g1 = sns.distplot(train['Sales'],hist = True,label='skewness:{:.2f}'.format(train['Sales'].skew()),ax = ax1)\n",
    "g1.legend()\n",
    "g1.set(xlabel = 'Sales', \n",
    "\n",
    "ylabel = 'Density', title = 'Sales Distribution')\n",
    "g2 = sns.distplot(np.log1p(train['Sales']), hist = True,label='skewness:{:.2f}'.format(np.log1p(train['Sales']).skew()),ax=ax2)\n",
    "g2.legend()\n",
    "g2.set(xlabel = 'log(Sales+1)',ylabel = 'Density', title = 'log(Sales+1) Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"StateHoliday\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process(data, isTest = False):\n",
    "    # label encode some features - this does not work. fixing it below\n",
    "    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "    # train.StoreType.replace(mappings, inplace=True)\n",
    "    # train.Assortment.replace(mappings, inplace=True)\n",
    "    data.StateHoliday.replace(mappings, inplace=True)\n",
    "    \n",
    "        \n",
    "    # extract some features from date column  \n",
    "    data['Month'] = data.Date.dt.month\n",
    "    data['Year'] = data.Date.dt.year\n",
    "    data['Day'] = data.Date.dt.day\n",
    "    data['WeekOfYear'] = data.Date.dt.weekofyear\n",
    "    \n",
    "    # calculate competiter open time in months\n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + \\\n",
    "        (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    data['CompetitionOpen'] = data['CompetitionOpen'].apply(lambda x: x if x > 0 else 0)\n",
    "    \n",
    "    # calculate promo2 open time in months\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + \\\n",
    "        (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n",
    "    data['PromoOpen'] = data['PromoOpen'].apply(lambda x: x if x > 0 else 0)\n",
    "                                                 \n",
    "    # Indicate whether the month is in promo interval\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \\\n",
    "             7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    data['month_str'] = data.Month.map(month2str)\n",
    "\n",
    "    def check(row):\n",
    "        if isinstance(row['PromoInterval'],str) and row['month_str'] in row['PromoInterval']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    data['IsPromoMonth'] =  data.apply(lambda row: check(row),axis=1)    \n",
    "    \n",
    "    # select the features we need\n",
    "    features = ['Store'\n",
    "                , 'DayOfWeek'\n",
    "                , 'Promo'\n",
    "                , 'StateHoliday'\n",
    "                , 'SchoolHoliday'\n",
    "                , 'StoreType'\n",
    "                , 'Assortment'\n",
    "                , 'CompetitionDistance'\n",
    "                , 'Promo2SinceWeek'\n",
    "                , 'Promo2SinceYear'\n",
    "                , 'Year'\n",
    "                , 'Month'\n",
    "                , 'Day'\n",
    "                , 'WeekOfYear'\n",
    "                , 'CompetitionOpen'\n",
    "                , 'PromoOpen'\n",
    "                , 'IsPromoMonth']  \n",
    "    if not isTest:\n",
    "        features.append('Sales')\n",
    "      \n",
    "       \n",
    "    data = data[features]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process(train)\n",
    "\n",
    "# sort by index\n",
    "train.sort_index(inplace = True)\n",
    "train_total.sort_index(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = process(valid)\n",
    "\n",
    "# sort by index\n",
    "valid.sort_index(inplace = True)\n",
    "# train_total.sort_index(inplace = True)\n",
    "\n",
    "categorical_columns = ['Store',\n",
    "                        'DayOfWeek',\n",
    "                        'Promo',\n",
    "                        'StateHoliday',\n",
    "                        'SchoolHoliday',\n",
    "                        'StoreType',\n",
    "                        'Assortment',\n",
    "                        # 'Year',\n",
    "                        # 'Month',\n",
    "                        # 'Day',\n",
    "                        # 'WeekOfYear',\n",
    "                        'IsPromoMonth']\n",
    "categorical_dims =  {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    print(col, valid[col].nunique())\n",
    "    valid[col] = valid[col].astype(str)\n",
    "    l_enc = LabelEncoder()\n",
    "    # train[col] = train[col].fillna(\"VV_likely\")\n",
    "    valid[col] = l_enc.fit_transform(valid[col].values)\n",
    "    # categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "    \n",
    "    \n",
    "print(categorical_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_all, y_valid_all = valid.drop(columns = ['Sales']), np.log1p(valid[['Sales']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "categorical_columns = ['Store',\n",
    "                        'DayOfWeek',\n",
    "                        'Promo',\n",
    "                        'StateHoliday',\n",
    "                        'SchoolHoliday',\n",
    "                        'StoreType',\n",
    "                        'Assortment',\n",
    "                        # 'Year',\n",
    "                        # 'Month',\n",
    "                        # 'Day',\n",
    "                        # 'WeekOfYear',\n",
    "                        'IsPromoMonth']\n",
    "categorical_dims =  {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    print(col, train[col].nunique())\n",
    "    train[col] = train[col].astype(str)\n",
    "    l_enc = LabelEncoder()\n",
    "    # train[col] = train[col].fillna(\"VV_likely\")\n",
    "    train[col] = l_enc.fit_transform(train[col].values)\n",
    "    # categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "    \n",
    "    \n",
    "print(categorical_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head().T.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split x and y\n",
    "X_all, y_all = train.drop(columns = ['Sales']), np.log1p(train[['Sales']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head().append(train.sample(5)).append(train.tail()).T.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_cats = X_all[categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of the number of unique values in each categorical column\n",
    "\n",
    "catmaxlist = [X_all_cats[col].nunique() for col in X_all_cats.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catmaxlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "target = 'Sales'\n",
    "unused_feat = ['Set']\n",
    "\n",
    "features = [ col for col in train.columns if col not in unused_feat+[target]] \n",
    "print('Feartures to be used', features)\n",
    "\n",
    "cat_dims = [X_all_cats[x].nunique() for x in X_all_cats.columns]\n",
    "print(cat_dims)\n",
    "\n",
    "# define your embedding sizes : here just a random choice\n",
    "cat_emb_dim =  [random.randint(2, min(x, 10)) for x in catmaxlist]\n",
    "print(cat_emb_dim)\n",
    "print(\"Categorical Dimensions: \", len(cat_dims))\n",
    "print(\"Categorical Embedding Dimensions: \", len(cat_emb_dim))\n",
    "assert len(cat_dims) == len(cat_emb_dim)\n",
    "\n",
    "print(\"Number of categorical features and embedding dimensions must be equal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Set\" not in train.columns:\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "    train[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(train.shape[0],))\n",
    "\n",
    "train_indices = train[train.Set==\"train\"].index\n",
    "valid_indices = train[train.Set==\"valid\"].index\n",
    "test_indices = train[train.Set==\"test\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all.values[train_indices].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_all.values[train_indices]\n",
    "y_train = y_all.values[train_indices].reshape(-1, 1)\n",
    "\n",
    "X_valid = X_all.values[valid_indices]\n",
    "y_valid = y_all.values[valid_indices].reshape(-1, 1)\n",
    "\n",
    "X_test = X_all.values[test_indices]\n",
    "y_test = y_all.values[test_indices].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head().T.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get categorical feature indexes \n",
    "\n",
    "cat_idxs = [X_all.columns.get_loc(c) for c in categorical_columns if c in X_all]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building model...')\n",
    "print('Number of features: {}'.format(X_train.shape[1]))\n",
    "print('Number of training samples: {}'.format(X_train.shape[0]))\n",
    "print('Number of categorical features: {}'.format(len(cat_dims)))\n",
    "print('Cat_dims: {}'.format(cat_dims))\n",
    "print('Indexes of categorical features: {}'.format(cat_idxs))\n",
    "print('List of categorical features and their embedding dimensions: {}'.format(list(zip(X_all_cats.columns, cat_emb_dim))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb\n",
    "\n",
    "sudo dpkg -i cuda-keyring_1.0-1_all.deb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Using {}\".format(DEVICE))\n",
    "\n",
    "clf = TabNetRegressor(cat_dims=cat_dims\n",
    "                      , cat_emb_dim=cat_emb_dim\n",
    "                      , cat_idxs=cat_idxs\n",
    "                      , device_name = DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 20 if not os.getenv(\"CI\", False) else 2\n",
    "print(max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['rmsle', 'mae', 'rmse', 'mse'],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=10,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    # num_workers=0,\n",
    "    # drop_last=False,\n",
    "    # augmentations=aug, #aug\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tabnet model\n",
    "saving_path_name = \"./tabnet_model_test_second_try\"\n",
    "saved_filepath = clf.save_model(saving_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new model with basic parameters and load state dict weights\n",
    "\n",
    "load_best = True\n",
    "\n",
    "if load_best:\n",
    "    saving_path_name = \"./tabnet_model.zip\"\n",
    "else:\n",
    "    saving_path_name = \"./tabnet_model_test_second_try.zip\"\n",
    "\n",
    "loaded_clf = TabNetRegressor()\n",
    "loaded_clf.load_model(saving_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out of time validation\n",
    "\n",
    "preds_valid = loaded_clf.predict(valid.values)\n",
    "valid2_mse = mean_squared_error(y_valid_all, preds_valid)\n",
    "\n",
    "\n",
    "print(f\"FINAL OUT OF TIME SCORE FOR {trainset_name} : {valid2_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot actual vs predicted in bins for validation set\n",
    "\n",
    "\n",
    "plt.hist(preds_valid, 100, alpha=0.5, label='preds')\n",
    "plt.hist(y_valid_all, 100, alpha=0.5, label='actual')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "print(f\"FINAL OUT OF TIME SCORE FOR {trainset_name} : {valid2_mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform back to original scale\n",
    "preds_valid_orig = np.expm1(preds_valid)\n",
    "y_valid_all_orig = np.expm1(y_valid_all)\n",
    "\n",
    "\n",
    "plt.hist(preds_valid_orig, 100, alpha=0.5, label='preds')\n",
    "plt.hist(y_valid_all_orig, 100, alpha=0.5, label='actual')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "valid2_mse_orig = mean_squared_error(y_valid_all_orig, preds_valid_orig, squared=False)\n",
    "\n",
    "\n",
    "print(f\"Original Scale - FINAL OUT OF TIME SCORE FOR {trainset_name} : {valid2_mse_orig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if loaded_cl\n",
    "preds = loaded_clf.predict(X_test)\n",
    "test_mse = mean_squared_error(preds, y_test)\n",
    "# else:\n",
    "loaded_preds = clf.predict(X_test)\n",
    "loaded_test_mse = mean_squared_error(loaded_preds, y_test)\n",
    "\n",
    "print(f\"FINAL TEST SCORE FOR {trainset_name} : {loaded_test_mse}\")\n",
    "print(f\"FINAL TEST SCORE FOR {trainset_name} : {test_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global explainability : feat importance summing to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local explainability and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix, masks = clf.explain(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, arr in masks.items():\n",
    "#     masks[key] = np.sort(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix_sorted = np.sort(explain_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.set_xticks([x for x in range(len(X_all.columns))])\n",
    "ax.set_xticklabels([x for x in X_all.columns], rotation=90)\n",
    "\n",
    "prev_val = explain_matrix_sorted[0][0] # set initial previous value\n",
    "sampled_rows = [explain_matrix_sorted[0]] # set initial sampled row\n",
    "for row in explain_matrix_sorted[1:]: # loop through remaining rows starting from the second row\n",
    "    curr_val = row[0]\n",
    "    if abs(curr_val - prev_val) > 0.005: # or any other threshold value you choose\n",
    "        sampled_rows.append(row)\n",
    "        if len(sampled_rows) == 100: # break out of loop after 100 rows have been sampled\n",
    "            break\n",
    "    prev_val = curr_val\n",
    "\n",
    "ax.imshow(sampled_rows)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.set_xticks([x for x in range(len(X_all.columns))])\n",
    "ax.set_xticklabels([x for x in X_all.columns], rotation=90)\n",
    "ax.imshow(explain_matrix[:50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.set_xticks([x for x in range(len(X_all.columns)+1)])\n",
    "ax.set_xticklabels([x for x in X_all.columns] + ['Prediction'], rotation=90)\n",
    "ax.imshow(np.append(explain_matrix[:50], y_test[:50].reshape(-1,1), axis=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explain_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].imshow(masks[i][:50])\n",
    "    axs[i].set_title(f\"mask {i}\")\n",
    "    axs[i].set_xticks([x for x in range(len(X_all.columns))])\n",
    "    axs[i].set_xticklabels([x for x in X_all.columns], rotation=90)\n",
    "\n",
    "#Can we sort this? \n",
    "#By feature importance and then weight for mask? #TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in X_all.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "clf_xgb = XGBRegressor(max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=1000,\n",
    "    verbosity=0,\n",
    "    silent=None,\n",
    "    objective='reg:linear',\n",
    "    booster='gbtree',\n",
    "    n_jobs=-1,\n",
    "    nthread=None,\n",
    "    gamma=0,\n",
    "    min_child_weight=1,\n",
    "    max_delta_step=0,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=1,\n",
    "    colsample_bylevel=1,\n",
    "    colsample_bynode=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    base_score=0.5,\n",
    "    random_state=0,\n",
    "    seed=None,)\n",
    "\n",
    "clf_xgb.fit(X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        early_stopping_rounds=40,\n",
    "        verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(clf_xgb.predict(X_valid))\n",
    "valid_mse = mean_squared_error(y_pred=preds, y_true=y_valid)\n",
    "print(valid_mse)\n",
    "\n",
    "preds = np.array(clf_xgb.predict(X_test))\n",
    "test_mse = mean_squared_error(y_pred=preds, y_true=y_test)\n",
    "print(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #make a graph of the feature imporances including their names\n",
    "\n",
    "feature_imp = pd.DataFrame(sorted(zip(clf_xgb.feature_importances_,X_all.columns)), columns=['Value','Feature'])\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('XGBoost Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL TEST SCORE FOR rossman-store-sales : 0.014900591809687539\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.014900591809687539/0.00897729870324486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvRossmann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "358e674b93cc6d0ab4918719be3a1bcc76ca2f2e76612c8e732cccbf67fe96bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
